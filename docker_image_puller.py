import os
import sys
import gzip
import json
import hashlib
import shutil
import threading
import time
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from tqdm import tqdm
import tarfile
import urllib3
import argparse
import logging
import base64
from concurrent.futures import ThreadPoolExecutor, as_completed

# Set default encoding to UTF-8
import io

sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# ç¦ç”¨ SSL è­¦å‘Š
urllib3.disable_warnings()

# ç‰ˆæœ¬å·
VERSION = "v1.2.0"

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s', encoding='utf-8')
logger = logging.getLogger(__name__)

stop_event = threading.Event()


def create_session():
    """åˆ›å»ºå¸¦æœ‰é‡è¯•å’Œä»£ç†é…ç½®çš„è¯·æ±‚ä¼šè¯"""
    session = requests.Session()

    # å¢å¼ºé‡è¯•ç­–ç•¥ï¼šæ›´å¤šé‡è¯•æ¬¡æ•°ï¼Œæ›´é•¿è¶…æ—¶
    retry_strategy = Retry(
        total=5,  # å¢åŠ é‡è¯•æ¬¡æ•°
        backoff_factor=2,  # æŒ‡æ•°é€€é¿ï¼š2, 4, 8, 16, 32ç§’
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["GET", "HEAD", "OPTIONS"]
    )

    # è®¾ç½®HTTPAdapterï¼Œæ›´é•¿è¶…æ—¶
    adapter = HTTPAdapter(
        max_retries=retry_strategy,
        pool_connections=10,  # è¿æ¥æ± å¤§å°
        pool_maxsize=20,  # æœ€å¤§è¿æ¥æ•°
        pool_block=False
    )

    session.mount("http://", adapter)
    session.mount("https://", adapter)

    # è®¾ç½®é»˜è®¤è¶…æ—¶
    session.timeout = (30, 300)  # (è¿æ¥è¶…æ—¶, è¯»å–è¶…æ—¶)

    # è®¾ç½®ä»£ç†
    session.proxies = {
        'http': os.environ.get('HTTP_PROXY') or os.environ.get('http_proxy'),
        'https': os.environ.get('HTTPS_PROXY') or os.environ.get('https_proxy')
    }
    if session.proxies.get('http') or session.proxies.get('https'):
        logger.info('ä½¿ç”¨ä»£ç†è®¾ç½®ä»ç¯å¢ƒå˜é‡')

    return session


def parse_image_input(args):
    """è§£æç”¨æˆ·è¾“å…¥çš„é•œåƒåç§°ï¼Œæ”¯æŒç§æœ‰ä»“åº“æ ¼å¼"""
    image_input = args.image
    # æ£€æŸ¥æ˜¯å¦åŒ…å«ç§æœ‰ä»“åº“åœ°å€
    if '/' in image_input and ('.' in image_input.split('/')[0] or ':' in image_input.split('/')[0]):
        # ç§æœ‰ä»“åº“æ ¼å¼: harbor.abc.com/abc/nginx:1.26.0
        registry, remainder = image_input.split('/', 1)
        parts = remainder.split('/')
        if len(parts) == 1:
            repo = ''
            img_tag = parts[0]
        else:
            repo = '/'.join(parts[:-1])
            img_tag = parts[-1]

        # è§£æé•œåƒåå’Œæ ‡ç­¾
        img, *tag_parts = img_tag.split(':')
        tag = tag_parts[0] if tag_parts else 'latest'

        # ç»„åˆæˆå®Œæ•´çš„ä»“åº“è·¯å¾„
        repository = remainder.split(':')[0]

        return registry, repository, img, tag
    else:
        # æ ‡å‡†Docker Hubæ ¼å¼
        parts = image_input.split('/')
        if len(parts) == 1:
            repo = 'library'
            img_tag = parts[0]
        else:
            repo = '/'.join(parts[:-1])
            img_tag = parts[-1]

        # è§£æé•œåƒåå’Œæ ‡ç­¾
        img, *tag_parts = img_tag.split(':')
        tag = tag_parts[0] if tag_parts else 'latest'

        # ç»„åˆæˆå®Œæ•´çš„ä»“åº“è·¯å¾„
        repository = f'{repo}/{img}'
        if not args.custom_registry:
            registry = 'registry-1.docker.io'
        else:
            registry = args.custom_registry
        return registry, repository, img, tag


def get_auth_head(session, auth_url, reg_service, repository, username=None, password=None):
    """è·å–è®¤è¯å¤´ï¼Œæ”¯æŒç”¨æˆ·åå¯†ç è®¤è¯"""
    try:
        url = f'{auth_url}?service={reg_service}&scope=repository:{repository}:pull'

        headers = {}
        # å¦‚æœæä¾›äº†ç”¨æˆ·åå’Œå¯†ç ï¼Œæ·»åŠ åˆ°è¯·æ±‚å¤´
        if username and password:
            auth_string = f"{username}:{password}"
            encoded_auth = base64.b64encode(auth_string.encode('utf-8')).decode('utf-8')
            headers['Authorization'] = f'Basic {encoded_auth}'

        # æ‰“å° curl å‘½ä»¤
        logger.debug(f"è·å–è®¤è¯å¤´ CURL å‘½ä»¤: curl '{url}'")

        resp = session.get(url, headers=headers, verify=False, timeout=30)
        resp.raise_for_status()
        access_token = resp.json()['token']
        auth_head = {'Authorization': f'Bearer {access_token}',
                     'Accept': 'application/vnd.docker.distribution.manifest.v2+json'}

        return auth_head
    except requests.exceptions.RequestException as e:
        logger.error(f'è¯·æ±‚è®¤è¯å¤±è´¥: {e}')
        raise


def fetch_manifest(session, registry, repository, tag, auth_head):
    """è·å–é•œåƒæ¸…å•"""
    try:
        url = f'https://{registry}/v2/{repository}/manifests/{tag}'
        # æ‰“å° curl å‘½ä»¤
        headers = ' '.join([f"-H '{key}: {value}'" for key, value in auth_head.items()])
        curl_command = f"curl '{url}' {headers}"
        logger.debug(f'è·å–é•œåƒæ¸…å• CURL å‘½ä»¤: {curl_command}')
        resp = session.get(url, headers=auth_head, verify=False, timeout=30)
        if resp.status_code == 401:
            logger.info('éœ€è¦è®¤è¯ã€‚')
            return resp, 401
        resp.raise_for_status()
        return resp, 200
    except requests.exceptions.RequestException as e:
        logger.error(f'è¯·æ±‚æ¸…å•å¤±è´¥: {e}')
        raise


def select_manifest(manifests, arch):
    """é€‰æ‹©é€‚åˆæŒ‡å®šæ¶æ„çš„æ¸…å•"""
    selected_manifest = None
    for m in manifests:
        if (m.get('annotations', {}).get('com.docker.official-images.bashbrew.arch') == arch or \
            m.get('platform', {}).get('architecture') == arch) and \
                m.get('platform', {}).get('os') == 'linux':
            selected_manifest = m.get('digest')
            break
    return selected_manifest


class DownloadProgressManager:
    """ä¸‹è½½è¿›åº¦ç®¡ç†å™¨ï¼Œæ”¯æŒè¿›åº¦æŒä¹…åŒ–"""

    def __init__(self, repository, tag, arch):
        """
        åˆå§‹åŒ–è¿›åº¦ç®¡ç†å™¨

        Args:
            repository: é•œåƒä»“åº“åï¼ˆå¦‚ï¼šlibrary/alpineï¼‰
            tag: é•œåƒæ ‡ç­¾ï¼ˆå¦‚ï¼šlatestï¼‰
            arch: æ¶æ„ï¼ˆå¦‚ï¼šamd64ï¼‰
        """
        self.repository = repository
        self.tag = tag
        self.arch = arch

        # ç”Ÿæˆå”¯ä¸€çš„è¿›åº¦æ–‡ä»¶å
        safe_repo = repository.replace("/", "_").replace(":", "_")
        self.progress_file = f'.download_progress_{safe_repo}_{tag}_{arch}.json'

        self.progress_data = self.load_progress()

    def load_progress(self):
        """åŠ è½½ä¸‹è½½è¿›åº¦ï¼Œå¹¶éªŒè¯é•œåƒä¿¡æ¯"""
        if os.path.exists(self.progress_file):
            try:
                with open(self.progress_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                    # éªŒè¯é•œåƒä¿¡æ¯æ˜¯å¦åŒ¹é…
                    metadata = data.get('metadata', {})
                    if (metadata.get('repository') == self.repository and
                            metadata.get('tag') == self.tag and
                            metadata.get('arch') == self.arch):

                        logger.info(f'ğŸ“‹ åŠ è½½å·²æœ‰ä¸‹è½½è¿›åº¦ï¼Œå…± {len(data.get("layers", {}))} ä¸ªæ–‡ä»¶')
                        return data
                    else:
                        logger.warning(f'è¿›åº¦æ–‡ä»¶é•œåƒä¿¡æ¯ä¸åŒ¹é…ï¼Œå°†åˆ›å»ºæ–°çš„è¿›åº¦')
                        logger.debug(
                            f'æ–‡ä»¶ä¸­: {metadata}, å½“å‰: {{repository: {self.repository}, tag: {self.tag}, arch: {self.arch}}}')
                        # é•œåƒä¿¡æ¯ä¸åŒ¹é…ï¼Œè¿”å›æ–°çš„è¿›åº¦æ•°æ®
                        return self._create_new_progress()

            except Exception as e:
                logger.warning(f'åŠ è½½è¿›åº¦æ–‡ä»¶å¤±è´¥: {e}')

        return self._create_new_progress()

    def _create_new_progress(self):
        """åˆ›å»ºæ–°çš„è¿›åº¦æ•°æ®ç»“æ„"""
        return {
            'metadata': {
                'repository': self.repository,
                'tag': self.tag,
                'arch': self.arch,
                'created_at': time.strftime('%Y-%m-%d %H:%M:%S')
            },
            'layers': {},
            'config': None
        }

    def save_progress(self):
        """ä¿å­˜ä¸‹è½½è¿›åº¦"""
        try:
            with open(self.progress_file, 'w', encoding='utf-8') as f:
                json.dump(self.progress_data, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.error(f'ä¿å­˜è¿›åº¦æ–‡ä»¶å¤±è´¥: {e}')

    def update_layer_status(self, digest, status, **kwargs):
        """æ›´æ–°å±‚çš„ä¸‹è½½çŠ¶æ€

        Args:
            digest: å±‚çš„digest
            status: çŠ¶æ€ (pending/downloading/completed/failed)
            **kwargs: å…¶ä»–ä¿¡æ¯ï¼ˆsize, downloadedç­‰ï¼‰
        """
        if digest not in self.progress_data['layers']:
            self.progress_data['layers'][digest] = {}

        self.progress_data['layers'][digest]['status'] = status
        self.progress_data['layers'][digest].update(kwargs)
        self.save_progress()

    def get_layer_status(self, digest):
        """è·å–å±‚çš„ä¸‹è½½çŠ¶æ€"""
        return self.progress_data['layers'].get(digest, {})

    def is_layer_completed(self, digest):
        """æ£€æŸ¥å±‚æ˜¯å¦å·²å®Œæˆä¸‹è½½"""
        layer_info = self.get_layer_status(digest)
        return layer_info.get('status') == 'completed'

    def update_config_status(self, status, **kwargs):
        """æ›´æ–°é…ç½®æ–‡ä»¶çŠ¶æ€"""
        if self.progress_data['config'] is None:
            self.progress_data['config'] = {}
        self.progress_data['config']['status'] = status
        self.progress_data['config'].update(kwargs)
        self.save_progress()

    def is_config_completed(self):
        """æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å·²å®Œæˆä¸‹è½½"""
        config_data = self.progress_data.get('config')
        if config_data is None:
            return False
        return config_data.get('status') == 'completed'

    def clear_progress(self):
        """æ¸…é™¤è¿›åº¦æ–‡ä»¶"""
        if os.path.exists(self.progress_file):
            try:
                os.remove(self.progress_file)
                logger.debug('è¿›åº¦æ–‡ä»¶å·²æ¸…é™¤')
            except Exception as e:
                logger.error(f'æ¸…é™¤è¿›åº¦æ–‡ä»¶å¤±è´¥: {e}')


def download_file_with_progress(session, url, headers, save_path, desc, expected_digest=None, max_retries=5):
    """
    ä¸‹è½½æ–‡ä»¶ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ ã€é‡è¯•å’Œæ ¡éªŒ

    Args:
        session: è¯·æ±‚ä¼šè¯
        url: ä¸‹è½½URL
        headers: è¯·æ±‚å¤´
        save_path: ä¿å­˜è·¯å¾„
        desc: è¿›åº¦æ¡æè¿°
        expected_digest: æœŸæœ›çš„SHA256æ‘˜è¦ï¼ˆå¯é€‰ï¼‰
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°

    Returns:
        bool: ä¸‹è½½æ˜¯å¦æˆåŠŸ
    """
    for attempt in range(max_retries):
        if stop_event.is_set():
            logger.info('ä¸‹è½½è¢«ç”¨æˆ·å–æ¶ˆ')
            return False

        # æ¯æ¬¡é‡è¯•æ—¶é‡æ–°è®¡ç®—å·²ä¸‹è½½çš„å¤§å°
        resume_pos = 0
        if os.path.exists(save_path):
            resume_pos = os.path.getsize(save_path)
            if resume_pos > 0:
                logger.info(f'{desc}: æ–­ç‚¹ç»­ä¼ ï¼Œä»ä½ç½® {resume_pos} å¼€å§‹')

        # æ·»åŠ Rangeå¤´å®ç°æ–­ç‚¹ç»­ä¼ 
        download_headers = headers.copy()
        if resume_pos > 0:
            download_headers['Range'] = f'bytes={resume_pos}-'

        try:
            with session.get(url, headers=download_headers, verify=False, timeout=60, stream=True) as resp:
                resp.raise_for_status()

                # è·å–æ€»å¤§å°
                content_range = resp.headers.get('content-range')
                if content_range:
                    total_size = int(content_range.split('/')[1])
                else:
                    total_size = int(resp.headers.get('content-length', 0)) + resume_pos

                # æ£€æŸ¥æ˜¯å¦éœ€è¦ç»­ä¼ 
                mode = 'ab' if resume_pos > 0 else 'wb'

                # åˆå§‹åŒ–SHA256è®¡ç®—å™¨
                sha256_hash = hashlib.sha256() if expected_digest else None

                # å¦‚æœæ˜¯æ–­ç‚¹ç»­ä¼ ä¸”éœ€è¦æ ¡éªŒï¼Œå…ˆè¯»å–å·²ä¸‹è½½éƒ¨åˆ†è®¡ç®—å“ˆå¸Œ
                if resume_pos > 0 and sha256_hash:
                    logger.debug(f'{desc}: è®¡ç®—å·²ä¸‹è½½éƒ¨åˆ†çš„æ ¡éªŒå’Œ...')
                    with open(save_path, 'rb') as existing_file:
                        while True:
                            chunk = existing_file.read(8192)
                            if not chunk:
                                break
                            sha256_hash.update(chunk)

                with open(save_path, mode) as file, tqdm(
                        total=total_size, initial=resume_pos, unit='B', unit_scale=True,
                        desc=desc, position=0, leave=True
                ) as pbar:
                    downloaded_size = resume_pos

                    for chunk in resp.iter_content(chunk_size=8192):
                        if stop_event.is_set():
                            logger.info('ä¸‹è½½è¢«ç”¨æˆ·å–æ¶ˆ')
                            return False

                        if chunk:
                            file.write(chunk)
                            pbar.update(len(chunk))
                            downloaded_size += len(chunk)

                            # å®æ—¶è®¡ç®—å“ˆå¸Œï¼ˆåŒ…å«æ–°ä¸‹è½½çš„éƒ¨åˆ†ï¼‰
                            if sha256_hash:
                                sha256_hash.update(chunk)

                # ä¸‹è½½å®Œæˆï¼ŒéªŒè¯å“ˆå¸Œ
                if expected_digest and sha256_hash:
                    actual_digest = f'sha256:{sha256_hash.hexdigest()}'
                    if actual_digest != expected_digest:
                        logger.error(f'âŒ {desc} æ ¡éªŒå¤±è´¥ï¼')
                        logger.error(f'æœŸæœ›: {expected_digest}')
                        logger.error(f'å®é™…: {actual_digest}')
                        logger.info(f'åˆ é™¤æŸåæ–‡ä»¶ï¼Œå‡†å¤‡é‡æ–°ä¸‹è½½...')

                        # åˆ é™¤æŸåçš„æ–‡ä»¶å¹¶é‡è¯•
                        if os.path.exists(save_path):
                            os.remove(save_path)

                        # ç­‰å¾…åé‡è¯•
                        if attempt < max_retries - 1:
                            wait_time = (2 ** attempt)
                            logger.info(f'ç­‰å¾… {wait_time} ç§’åé‡è¯•...')
                            time.sleep(wait_time)
                        continue  # é‡è¯•

                    logger.info(f'âœ… {desc} æ ¡éªŒæˆåŠŸ')

                logger.info(f'âœ… {desc} ä¸‹è½½å®Œæˆ')
                return True

        except KeyboardInterrupt:
            logger.info(f'âš ï¸  ä¸‹è½½ {url} è¢«ç”¨æˆ·å–æ¶ˆ')
            if os.path.exists(save_path):
                os.remove(save_path)
            return False
        except (requests.exceptions.Timeout, requests.exceptions.ConnectionError) as e:
            logger.warning(f'âš ï¸  {desc} ç¬¬ {attempt + 1}/{max_retries} æ¬¡ä¸‹è½½è¶…æ—¶: {e}')
            if attempt < max_retries - 1:
                wait_time = (2 ** attempt)  # æŒ‡æ•°é€€é¿ï¼š2, 4, 8, 16, 32ç§’
                logger.info(f'ç­‰å¾… {wait_time} ç§’åé‡è¯•...')
                time.sleep(wait_time)
                continue
            else:
                logger.error(f'âŒ {desc} ä¸‹è½½å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°')
                if os.path.exists(save_path):
                    os.remove(save_path)
                return False
        except requests.exceptions.HTTPError as e:
            logger.warning(f'âš ï¸  {desc} HTTPé”™è¯¯: {e}')
            if e.response.status_code in [429, 500, 502, 503, 504] and attempt < max_retries - 1:
                wait_time = (2 ** attempt)
                logger.info(f'ç­‰å¾… {wait_time} ç§’åé‡è¯•...')
                time.sleep(wait_time)
                continue
            else:
                logger.error(f'âŒ {desc} ä¸‹è½½å¤±è´¥: {e}')
                if os.path.exists(save_path):
                    os.remove(save_path)
                return False
        except Exception as e:
            logger.error(f'âŒ {desc} ä¸‹è½½å¤±è´¥: {e}')
            if os.path.exists(save_path):
                os.remove(save_path)
            return False

    return False


def download_layers(session, registry, repository, layers, auth_head, imgdir, resp_json, imgparts, img, tag, arch):
    """å¤šçº¿ç¨‹ä¸‹è½½é•œåƒå±‚ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ ã€é‡è¯•å’Œæ ¡éªŒ"""
    os.makedirs(imgdir, exist_ok=True)

    # åˆ›å»ºè¿›åº¦ç®¡ç†å™¨ï¼ˆæ¯ä¸ªé•œåƒç‹¬ç«‹çš„è¿›åº¦æ–‡ä»¶ï¼‰
    progress_manager = DownloadProgressManager(repository, tag, arch)

    try:
        config_digest = resp_json['config']['digest']
        config_filename = f'{config_digest[7:]}.json'
        config_path = os.path.join(imgdir, config_filename)
        config_url = f'https://{registry}/v2/{repository}/blobs/{config_digest}'

        # æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å·²ä¸‹è½½å®Œæˆ
        if progress_manager.is_config_completed() and os.path.exists(config_path):
            logger.info(f'âœ… Config {config_filename} å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½')
        else:
            logger.debug(f'ä¸‹è½½ Config: {config_filename}')
            progress_manager.update_config_status('downloading', digest=config_digest)

            # ä¸‹è½½é…ç½®æ–‡ä»¶å¹¶è¿›è¡Œæ ¡éªŒ
            if not download_file_with_progress(session, config_url, auth_head, config_path, "Config",
                                               expected_digest=config_digest):
                progress_manager.update_config_status('failed')
                raise Exception(f'Config JSON {config_filename} ä¸‹è½½å¤±è´¥')

            progress_manager.update_config_status('completed', digest=config_digest)

    except Exception as e:
        logging.error(f'è¯·æ±‚é…ç½®å¤±è´¥: {e}')
        return

    repo_tag = f'{"/".join(imgparts)}/{img}:{tag}' if imgparts else f'{img}:{tag}'
    content = [{'Config': config_filename, 'RepoTags': [repo_tag], 'Layers': []}]
    parentid = ''
    layer_json_map = {}

    # ç»Ÿè®¡éœ€è¦ä¸‹è½½çš„å±‚
    layers_to_download = []
    skipped_count = 0

    for layer in layers:
        ublob = layer['digest']
        fake_layerid = hashlib.sha256((parentid + '\n' + ublob + '\n').encode('utf-8')).hexdigest()
        layerdir = f'{imgdir}/{fake_layerid}'
        os.makedirs(layerdir, exist_ok=True)
        layer_json_map[fake_layerid] = {"id": fake_layerid, "parent": parentid if parentid else None}
        parentid = fake_layerid

        save_path = f'{layerdir}/layer_gzip.tar'

        # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆä¸‹è½½
        if progress_manager.is_layer_completed(ublob) and os.path.exists(save_path):
            logger.info(f'âœ… å±‚ {ublob[:12]} å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½')
            skipped_count += 1
        else:
            layers_to_download.append((ublob, fake_layerid, layerdir, save_path))

    if skipped_count > 0:
        logger.info(f'ğŸ“¦ è·³è¿‡ {skipped_count} ä¸ªå·²ä¸‹è½½çš„å±‚ï¼Œè¿˜éœ€ä¸‹è½½ {len(layers_to_download)} ä¸ªå±‚')

    # å¤šçº¿ç¨‹ä¸‹è½½éœ€è¦ä¸‹è½½çš„å±‚
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = {}
        try:
            for ublob, fake_layerid, layerdir, save_path in layers_to_download:
                if stop_event.is_set():
                    raise KeyboardInterrupt  # æ£€æµ‹åˆ°ç»ˆæ­¢ä¿¡å·

                url = f'https://{registry}/v2/{repository}/blobs/{ublob}'

                # æ ‡è®°ä¸ºä¸‹è½½ä¸­
                progress_manager.update_layer_status(ublob, 'downloading')

                # ä¼ é€’digestè¿›è¡Œæ ¡éªŒ
                futures[executor.submit(
                    download_file_with_progress,
                    session,
                    url,
                    auth_head,
                    save_path,
                    ublob[:12],
                    expected_digest=ublob
                )] = (ublob, save_path)

            for future in as_completed(futures):
                if stop_event.is_set():
                    raise KeyboardInterrupt  # é€€å‡º

                ublob, save_path = futures[future]
                result = future.result()

                if not result:
                    progress_manager.update_layer_status(ublob, 'failed')
                    raise Exception(f'å±‚ {ublob[:12]} ä¸‹è½½å¤±è´¥')
                else:
                    progress_manager.update_layer_status(ublob, 'completed')

        except KeyboardInterrupt:
            logging.error("ç”¨æˆ·ç»ˆæ­¢ä¸‹è½½ï¼Œä¿å­˜å½“å‰è¿›åº¦...")
            stop_event.set()  # è®¾ç½®ç»ˆæ­¢æ ‡å¿—
            executor.shutdown(wait=False)
            # ä¸åˆ é™¤éƒ¨åˆ†ä¸‹è½½çš„æ–‡ä»¶ï¼Œä¿ç•™ç”¨äºæ–­ç‚¹ç»­ä¼ 
            raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©å¤–å±‚å¤„ç†

    # è§£å‹å’Œå¤„ç†æ‰€æœ‰å±‚
    for fake_layerid in layer_json_map.keys():
        if stop_event.is_set():
            # æ£€æµ‹åˆ°ç»ˆæ­¢ä¿¡å·ï¼Œæå‰é€€å‡º
            raise KeyboardInterrupt("ç”¨æˆ·å·²å–æ¶ˆæ“ä½œ")

        layerdir = f'{imgdir}/{fake_layerid}'
        gz_path = f'{layerdir}/layer_gzip.tar'
        tar_path = f'{layerdir}/layer.tar'

        # è§£å‹gzipæ–‡ä»¶
        if os.path.exists(gz_path):
            with gzip.open(gz_path, 'rb') as gz, open(tar_path, 'wb') as file:
                shutil.copyfileobj(gz, file)
            os.remove(gz_path)

        json_path = f'{layerdir}/json'
        with open(json_path, 'w') as file:
            json.dump(layer_json_map[fake_layerid], file)

        content[0]['Layers'].append(f'{fake_layerid}/layer.tar')

    manifest_path = os.path.join(imgdir, 'manifest.json')
    with open(manifest_path, 'w') as file:
        json.dump(content, file)

    repositories_path = os.path.join(imgdir, 'repositories')
    with open(repositories_path, 'w') as file:
        json.dump({repository if '/' in repository else img: {tag: parentid}}, file)

    logging.info(f'âœ… é•œåƒ {img}:{tag} ä¸‹è½½å®Œæˆï¼')

    # æ¸…é™¤è¿›åº¦æ–‡ä»¶
    progress_manager.clear_progress()


def create_image_tar(imgdir, repository, tag, arch):
    """å°†é•œåƒæ‰“åŒ…ä¸º tar æ–‡ä»¶"""
    safe_repo = repository.replace("/", "_")
    docker_tar = f'{safe_repo}_{tag}_{arch}.tar'
    try:
        with tarfile.open(docker_tar, "w") as tar:
            tar.add(imgdir, arcname='/')
        logger.debug(f'Docker é•œåƒå·²æ‹‰å–ï¼š{docker_tar}')
        return docker_tar
    except Exception as e:
        logger.error(f'æ‰“åŒ…é•œåƒå¤±è´¥: {e}')
        raise


def cleanup_tmp_dir():
    """åˆ é™¤ tmp ç›®å½•"""
    tmp_dir = 'tmp'
    try:
        if os.path.exists(tmp_dir):
            logger.debug(f'æ¸…ç†ä¸´æ—¶ç›®å½•: {tmp_dir}')
            shutil.rmtree(tmp_dir)
            logger.debug('ä¸´æ—¶ç›®å½•å·²æ¸…ç†ã€‚')
    except Exception as e:
        logger.error(f'æ¸…ç†ä¸´æ—¶ç›®å½•å¤±è´¥: {e}')


def main():
    """ä¸»å‡½æ•°"""
    try:
        parser = argparse.ArgumentParser(description="Docker é•œåƒæ‹‰å–å·¥å…·")
        parser.add_argument("-i", "--image", required=False,
                            help="Docker é•œåƒåç§°ï¼ˆä¾‹å¦‚ï¼šnginx:latest æˆ– harbor.abc.com/abc/nginx:1.26.0ï¼‰")
        parser.add_argument("-q", "--quiet", action="store_true", help="é™é»˜æ¨¡å¼ï¼Œå‡å°‘äº¤äº’")
        parser.add_argument("-r", "--custom_registry", help="è‡ªå®šä¹‰ä»“åº“åœ°å€ï¼ˆä¾‹å¦‚ï¼šharbor.abc.comï¼‰")
        parser.add_argument("-a", "--arch", help="æ¶æ„,é»˜è®¤ï¼šamd64,å¸¸è§ï¼šamd64, arm64v8ç­‰")
        parser.add_argument("-u", "--username", help="Docker ä»“åº“ç”¨æˆ·å")
        parser.add_argument("-p", "--password", help="Docker ä»“åº“å¯†ç ")
        parser.add_argument("-v", "--version", action="version", version=f"%(prog)s {VERSION}", help="æ˜¾ç¤ºç‰ˆæœ¬ä¿¡æ¯")
        parser.add_argument("--debug", action="store_true", help="å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œæ‰“å°è¯·æ±‚ URL å’Œè¿æ¥çŠ¶æ€")

        # æ˜¾ç¤ºç¨‹åºçš„ä¿¡æ¯
        logger.info(f'æ¬¢è¿ä½¿ç”¨ Docker é•œåƒæ‹‰å–å·¥å…· {VERSION}')

        args = parser.parse_args()

        if args.debug:
            logger.setLevel(logging.DEBUG)

        # è·å–é•œåƒåç§°
        if not args.image:
            args.image = input("è¯·è¾“å…¥ Docker é•œåƒåç§°ï¼ˆä¾‹å¦‚ï¼šnginx:latest æˆ– harbor.abc.com/abc/nginx:1.26.0ï¼‰ï¼š").strip()
            if not args.image:
                logger.error("é”™è¯¯ï¼šé•œåƒåç§°æ˜¯å¿…å¡«é¡¹ã€‚")
                return

        # # è·å–æ¶æ„
        # if not args.arch and not args.quiet:
        #     args.arch = input("è¯·è¾“å…¥æ¶æ„ï¼ˆå¸¸è§: amd64, arm64v8ç­‰ï¼Œé»˜è®¤: amd64ï¼‰ï¼š").strip() or 'amd64'

        # è·å–è‡ªå®šä¹‰ä»“åº“åœ°å€
        if not args.custom_registry and not args.quiet:
            # use_custom_registry = input("æ˜¯å¦ä½¿ç”¨è‡ªå®šä¹‰ä»“åº“åœ°å€ï¼Ÿ(y/n, é»˜è®¤: y): ").strip().lower() or 'y'
            # if use_custom_registry == 'y':
            #     args.custom_registry = input("è¯·è¾“å…¥è‡ªå®šä¹‰ä»“åº“åœ°å€: )").strip()
            args.custom_registry = input("è¯·è¾“å…¥è‡ªå®šä¹‰ä»“åº“åœ°å€: ï¼ˆé»˜è®¤ dockerhubï¼‰").strip()

        # è§£æé•œåƒä¿¡æ¯
        registry, repository, img, tag = parse_image_input(args)

        # è·å–è®¤è¯ä¿¡æ¯
        if not args.username and not args.quiet:
            args.username = input("è¯·è¾“å…¥é•œåƒä»“åº“ç”¨æˆ·å: ").strip()
        if not args.password and not args.quiet:
            args.password = input("è¯·è¾“å…¥é•œåƒä»“åº“å¯†ç : ").strip()
        session = create_session()
        auth_head = None
        try:
            url = f'https://{registry}/v2/'
            logger.debug(f"è·å–è®¤è¯ä¿¡æ¯ CURL å‘½ä»¤: curl '{url}'")
            resp = session.get(url, verify=False, timeout=30)
            auth_url = resp.headers['WWW-Authenticate'].split('"')[1]
            reg_service = resp.headers['WWW-Authenticate'].split('"')[3]
            auth_head = get_auth_head(session, auth_url, reg_service, repository, args.username, args.password)
            # è·å–æ¸…å•
            resp, http_code = fetch_manifest(session, registry, repository, tag, auth_head)
            if http_code == 401:
                use_auth = input(f"å½“å‰ä»“åº“ {registry}ï¼Œéœ€è¦ç™»å½•ï¼Ÿ(y/n, é»˜è®¤: y): ").strip().lower() or 'y'
                if use_auth == 'y':
                    args.username = input("è¯·è¾“å…¥ç”¨æˆ·å: ").strip()
                    args.password = input("è¯·è¾“å…¥å¯†ç : ").strip()
                auth_head = get_auth_head(session, auth_url, reg_service, repository, args.username, args.password)

            resp, http_code = fetch_manifest(session, registry, repository, tag, auth_head)
        except requests.exceptions.RequestException as e:
            logger.error(f'è¿æ¥ä»“åº“å¤±è´¥: {e}')
            raise

        resp_json = resp.json()

        # å¤„ç†å¤šæ¶æ„é•œåƒ
        manifests = resp_json.get('manifests')
        if manifests is not None:
            archs = [m.get('annotations', {}).get('com.docker.official-images.bashbrew.arch') or
                     m.get('platform', {}).get('architecture')
                     for m in manifests if m.get('platform', {}).get('os') == 'linux']

            # æ‰“å°æ¶æ„åˆ—è¡¨
            if archs:
                logger.debug(f'å½“å‰å¯ç”¨æ¶æ„ï¼š{", ".join(archs)}')

            if len(archs) == 1:
                args.arch = archs[0]
                logger.info(f'è‡ªåŠ¨é€‰æ‹©å”¯ä¸€å¯ç”¨æ¶æ„: {args.arch}')

            # è·å–æ¶æ„
            if not args.arch or args.arch not in archs:
                args.arch = input(f"è¯·è¾“å…¥æ¶æ„ï¼ˆå¯é€‰: {', '.join(archs)}ï¼Œé»˜è®¤: amd64ï¼‰ï¼š").strip() or 'amd64'

            digest = select_manifest(manifests, args.arch)
            if not digest:
                logger.error(f'åœ¨æ¸…å•ä¸­æ‰¾ä¸åˆ°æŒ‡å®šçš„æ¶æ„ {args.arch}')
                return

            # æ„é€ è¯·æ±‚
            url = f'https://{registry}/v2/{repository}/manifests/{digest}'
            headers = ' '.join([f"-H '{key}: {value}'" for key, value in auth_head.items()])
            curl_command = f"curl '{url}' {headers}"
            logger.debug(f'è·å–æ¶æ„æ¸…å• CURL å‘½ä»¤: {curl_command}')

            # è·å–æ¸…å•
            manifest_resp = session.get(url, headers=auth_head, verify=False, timeout=30)
            try:
                manifest_resp.raise_for_status()
                resp_json = manifest_resp.json()
            except Exception as e:
                logger.error(f'è·å–æ¶æ„æ¸…å•å¤±è´¥: {e}')
                return

            if 'layers' not in resp_json:
                logger.error('é”™è¯¯ï¼šæ¸…å•ä¸­æ²¡æœ‰å±‚')
                return

            if 'config' not in resp_json:
                logger.error('é”™è¯¯ï¼šæ¸…å•ä¸­æ²¡æœ‰é…ç½®ä¿¡æ¯')
                return

        # æœ€ç»ˆæ£€æŸ¥ï¼šç¡®ä¿æ¸…å•å®Œæ•´
        if 'layers' not in resp_json or 'config' not in resp_json:
            logger.error('é”™è¯¯ï¼šæ¸…å•æ ¼å¼ä¸å®Œæ•´ï¼Œç¼ºå°‘å¿…è¦å­—æ®µ')
            logger.debug(f'æ¸…å•å†…å®¹: {resp_json.keys()}')
            return

        logger.info(f'ä»“åº“åœ°å€ï¼š{registry}')
        logger.info(f'é•œåƒï¼š{repository}')
        logger.info(f'æ ‡ç­¾ï¼š{tag}')
        logger.info(f'æ¶æ„ï¼š{args.arch}')

        # ä¸‹è½½é•œåƒå±‚
        imgdir = 'tmp'
        os.makedirs(imgdir, exist_ok=True)
        logger.info('å¼€å§‹ä¸‹è½½')

        # æ ¹æ®é•œåƒç±»å‹ï¼Œæä¾›æ­£ç¡®çš„imgparts
        if registry == 'registry-1.docker.io' and repository.startswith('library/'):
            # Docker Hub
            imgparts = []  # å®˜æ–¹é•œåƒä¸éœ€è¦å‰ç¼€
        else:
            #
            imgparts = repository.split('/')[:-1]

        download_layers(session, registry, repository, resp_json['layers'], auth_head, imgdir, resp_json, imgparts, img,
                        tag, args.arch)

        # æ‰“åŒ…é•œåƒ
        output_file = create_image_tar(imgdir, repository, tag, args.arch)
        logger.info(f'é•œåƒå·²ä¿å­˜ä¸º: {output_file}')
        logger.info(f'å¯ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯¼å…¥é•œåƒ: docker load -i {output_file}')
        if registry not in ("registry-1.docker.io", "docker.io"):
            logger.info(f'æ‚¨å¯èƒ½éœ€è¦: docker tag {repository}:{tag} {registry}/{repository}:{tag}')



    except KeyboardInterrupt:
        logger.info('ç”¨æˆ·å–æ¶ˆæ“ä½œã€‚')
    except requests.exceptions.RequestException as e:
        logger.error(f'ç½‘ç»œè¿æ¥å¤±è´¥: {e}')
    except json.JSONDecodeError as e:
        logger.error(f'JSONè§£æå¤±è´¥: {e}')
    except FileNotFoundError as e:
        logger.error(f'æ–‡ä»¶æ“ä½œå¤±è´¥: {e}')
    except argparse.ArgumentError as e:
        logger.error(f'å‘½ä»¤è¡Œå‚æ•°é”™è¯¯: {e}')
    except Exception as e:
        logger.error(f'ç¨‹åºè¿è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}')
        import traceback
        logger.debug(traceback.format_exc())

    finally:
        cleanup_tmp_dir()
        try:
            input("æŒ‰ä»»æ„é”®é€€å‡ºç¨‹åº...")
        except (KeyboardInterrupt, EOFError):
            # ç”¨æˆ·æŒ‰Ctrl+Cæˆ–åœ¨éäº¤äº’ç¯å¢ƒä¸­è¿è¡Œ
            pass
        sys.exit(0)


if __name__ == '__main__':
    main()